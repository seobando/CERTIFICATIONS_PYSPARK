{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICE TEST II: questions to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sobando\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The below Dataframe's need to joined with each other in same order below and the output should display all records from dfDept which has no match in the dfEmp Dataframe. What is the appropriate join option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "dfDept = spark.createDataFrame([\n",
    "\n",
    "                    ['Sls','Sales'],\n",
    "\n",
    "                    ['Admin','Administration'],\n",
    "\n",
    "                    ['IT','Information Tech']],('dept','dept_name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|name|dept|salary|\n",
      "+----+----+------+\n",
      "|Tony|  HR|  8000|\n",
      "|Mona|null| 10000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp.join(dfDept,dfEmp.dept == dfDept.dept,how=\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| Tim|Admin|  7000|\n",
      "|Jill|  Sls|  5000|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp.join(dfDept,dfEmp.dept == dfDept.dept,how=\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmp.createOrReplaceTempView('Emp')\n",
    "dfDept.createOrReplaceTempView('Dept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|name|dept|salary|\n",
      "+----+----+------+\n",
      "|Tony|  HR|  8000|\n",
      "|Mona|null| 10000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL script\n",
    "sql = \"SELECT * FROM Emp ANTI JOIN Dept ON Emp.dept = Dept.dept;\" \n",
    "# Spark SQL\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|name|dept|salary|\n",
      "+----+----+------+\n",
      "|Tony|  HR|  8000|\n",
      "|Mona|null| 10000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL script\n",
    "sql = \"SELECT * FROM Emp ANTI JOIN Dept ON Emp.dept = Dept.dept;\" \n",
    "# Spark SQL\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference = https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records will the below query display?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+----------------+\n",
      "| dept|name|salary|       dept_name|\n",
      "+-----+----+------+----------------+\n",
      "|   HR|Tony|  8000|            null|\n",
      "| null|Mona| 10000|            null|\n",
      "|Admin| Tim|  7000|  Administration|\n",
      "|   IT|null|  null|Information Tech|\n",
      "|  Sls|Jill|  5000|           Sales|\n",
      "+-----+----+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "dfDept = spark.createDataFrame([\n",
    "\n",
    "                    ['Sls','Sales'],\n",
    "\n",
    "                    ['Admin','Administration'],\n",
    "\n",
    "                    ['IT','Information Tech']],('dept','dept_name'))\n",
    "\n",
    "df = dfEmp.join(dfDept,\"dept\",\"outer\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+--------------+\n",
      "| dept|name|salary|     dept_name|\n",
      "+-----+----+------+--------------+\n",
      "|Admin| Tim|  7000|Administration|\n",
      "|  Sls|Jill|  5000|         Sales|\n",
      "+-----+----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.na.drop(\"any\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A source data file has 5 columns. One of the columns has bad data and will cause the read to fail for a few records. In order to ensure that the file reads successfully for the good quality records and keep track of the bad data to analyze further, what is the appropriate option below?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columnNameOfCorruptRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The source data to be read into a DataFrame exists in a SQL Database. In order to reduce the number of records processed by the Dataframe, we need to reduce the number of records sourced from the read action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set upperbpund, lowerbound, numPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How many columns will the write command output in CSV file, based on last line of code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/sobando/Downloads/SPARK_PRACTICE/empDetails.csv already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-6d99adb1a39a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdfEmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfEmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dept\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdfEmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1025\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1027\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/sobando/Downloads/SPARK_PRACTICE/empDetails.csv already exists.;"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "path=\"empDetails.csv\"\n",
    "\n",
    "dfEmp = dfEmp.withColumn(\"commision\",col(\"salary\")*1.03)\n",
    "\n",
    "dfEmp.write.csv(path)\n",
    "\n",
    "dfEmp = dfEmp.drop(\"dept\")\n",
    "\n",
    "dfEmp.write.csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. An Retail company processes its sales in Apache Spark on a daily batch basis. Each day, the sales is added into \"sales_details.parquet\" file that contains historical and on-going daily sales. what is the most appropriate compression option for parquet files to have optimal performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "No es claro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. There is a Dataframe that has 30+ Columns. For further analysis, we need a specific set of 3 columns to be selected for further analysis. All options given below are suitable for the requirement except one. Choose the invalid option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.select(cols(\"fname\",\"lname\",\"sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Choose the correct option to display the contents of the Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "cust = Row(\"fname\",\"lname\")\n",
    "\n",
    "custRow = cust(\"John\",\"Doe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('John', 'Doe')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custRow[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. The Dataframe df consists of information related to sales dept. Due to sales team hitting their targets, the management has provided a directive to assign 5% commission. Select the appropriate option from below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['John','Doe','25','2800'],\n",
    "\n",
    "['Joe','Smith','24','2600']],('fname','lname','age','sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-----+---------+\n",
      "|fname|lname|age|sales|sales_com|\n",
      "+-----+-----+---+-----+---------+\n",
      "| John|  Doe| 25| 2800|    140.0|\n",
      "|  Joe|Smith| 24| 2600|    130.0|\n",
      "+-----+-----+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df.withColumn(\"sales_com\",col(\"sales\").cast(IntegerType())*0.05).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. There are different methods to create a Dataframe in order run small tests. One of the options below will not provide the desired output. Choose the incorrect option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['John','Boston','john@mailcom'],\n",
    "    ['Smith','New York','smith@mail.com']\n",
    "],('name','city','email'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------+\n",
      "| name|    city|         email|\n",
      "+-----+--------+--------------+\n",
      "| John|  Boston|  john@mailcom|\n",
      "|Smith|New York|smith@mail.com|\n",
      "+-----+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['John','Boston','john@mailcom'],\n",
    "    ['Smith','New York','smith@mail.com']\n",
    "]).toDF('name','city','email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------+\n",
      "| name|    city|         email|\n",
      "+-----+--------+--------------+\n",
      "| John|  Boston|  john@mailcom|\n",
      "|Smith|New York|smith@mail.com|\n",
      "+-----+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "withColumnRenamed() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e04d3e1b686d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df = spark.createDataFrame([\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;34m'John'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Boston'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'john@mailcom'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;34m'Smith'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'New York'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'smith@mail.com'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ]).withColumnRenamed(col('name'),col('city'),col('email'))\n",
      "\u001b[1;31mTypeError\u001b[0m: withColumnRenamed() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['John','Boston','john@mailcom'],\n",
    "    ['Smith','New York','smith@mail.com']\n",
    "]).withColumnRenamed(col('name'),col('city'),col('email'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=['name','city','email']\n",
    "df = spark.createDataFrame([\n",
    "    ['John','Boston','john@mailcom'],\n",
    "    ['Smith','New York','smith@mail.com']\n",
    "],lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------+\n",
      "| name|    city|         email|\n",
      "+-----+--------+--------------+\n",
      "| John|  Boston|  john@mailcom|\n",
      "|Smith|New York|smith@mail.com|\n",
      "+-----+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. As part of existing Spark Code, there is a Global View \"gvSales\" that has been created. In order to select data from the global view, choose the appropriate option from below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sql(\"select * from gvSales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. The number of tables have grown manifold in the Test Environment, due to various teams creating their own versions for testing different concepts. The QA Lead is tasked with finding all the table names to share with the team to take appropriate action. Choose the valid option from below to find the list of tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-c6414f168cc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dept', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='emp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Catalog' object has no attribute 'databases'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-0760b736c0ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Catalog' object has no attribute 'databases'"
     ]
    }
   ],
   "source": [
    "spark.catalog.databases(tables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-1324aecec531>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "spark.show(tables=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. An Email Marketing company has a requirement to check the length of COMMENTS field before it sends the final file to partner organizations. The options given below will all work except one. Find the invalid option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.selectEval(\"length(comments)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. In order to improve readability, you have been tasked to rename the outputs of your select commands. Choose the correct option below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.select(col(\"name\").alias(\"employee_name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Find the lowest salary in the below DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|name|  city| salary|\n",
      "+----+------+-------+\n",
      "| Bob|Boston|15300.0|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['Bob','Boston',10000],\n",
    "\n",
    "['Will','New York',12000],\n",
    "\n",
    "                        ['Lisa','Detroit',13000],\n",
    "\n",
    "                        ['Zoe','Detroit',None],],('name','city','salary'))\n",
    "\n",
    "\n",
    "\n",
    "df = df.where(df.salary.isNotNull())\n",
    "\n",
    "df = df.withColumn(\"salary\",col(\"salary\")*1.02)\n",
    "\n",
    "df = df.where(df.name == 'Bob').withColumn(\"salary\",col(\"salary\")*1.5)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. How many records are displayed after executing the below code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|  Joe| 25|\n",
      "|Smith| 27|\n",
      "| null| 28|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['Joe',25],\n",
    "\n",
    "['Smith',27],\n",
    "\n",
    "                        [None,28],\n",
    "\n",
    "                        ['Sharon',33]],('name','age'))\n",
    "\n",
    "df = df.filter(col(\"age\") < 30).filter(col(\"age\") > 20)\n",
    "\n",
    "df.filter(col(\"name\").isNotNull())\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Identify the output of the code. Whose name comes up in the first Row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-62598ed167a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   2094\u001b[0m         \"\"\"\n\u001b[0;32m   2095\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0margs_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1260\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1261\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   1245\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m                         \u001b[0mtemp_arg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1248\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, object, gateway_client)\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"java.util.ArrayList\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[0mjava_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjava_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Column is not iterable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;31m# string methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "                    ['Joe',25],\n",
    "\n",
    "                    ['Smith',27],\n",
    "\n",
    "                    [None,28],\n",
    "\n",
    "                    ['Sharon',33]],('name','score'))\n",
    "\n",
    "df = df.filter(col(\"score\") < 30).filter(col(\"score\") > 20)\n",
    "\n",
    "df = df.withColumn(col(\"score\"),col(\"score\") * 1.8)\n",
    "\n",
    "df = df.select(\"*\").orderBy(col(\"score\").desc())\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Identify the output of the code. Whose name comes up in the first Row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  name|score|\n",
      "+------+-----+\n",
      "|   Joe|   25|\n",
      "| Smith|   27|\n",
      "|  null|   28|\n",
      "|Sharon|   33|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "                    ['Joe',25],\n",
    "\n",
    "                    ['Smith',27],\n",
    "\n",
    "                    [None,28],\n",
    "\n",
    "                    ['Sharon',33]],('name','score'))\n",
    "\n",
    "df.filter(col(\"name\") != 'Smith')\n",
    "\n",
    "df.select(\"*\").orderBy(col(\"score\").desc_nulls_last())\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. What is the output of the below code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+\n",
      "|          dept|designation|sum(sales)|\n",
      "+--------------+-----------+----------+\n",
      "|          null|       null|    105000|\n",
      "|          null|     Jr Rep|     30000|\n",
      "|          null|     Sr Rep|     75000|\n",
      "|SALES - ENERGY|       null|     43000|\n",
      "|SALES - ENERGY|     Jr Rep|     18000|\n",
      "|SALES - ENERGY|     Sr Rep|     25000|\n",
      "|SALES - RETAIL|       null|     62000|\n",
      "|SALES - RETAIL|     Jr Rep|     12000|\n",
      "|SALES - RETAIL|     Sr Rep|     50000|\n",
      "+--------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "                    ['Will','SALES - ENERGY','Sr Rep',25000],\n",
    "\n",
    "                    ['Tom','SALES - RETAIL','Sr Rep',50000],\n",
    "\n",
    "                    ['Bill','SALES - RETAIL','Jr Rep',12000],\n",
    "\n",
    "                    ['Vanessa','SALES - ENERGY','Jr Rep',18000]],('name','dept','designation','sales'))\n",
    "\n",
    "df.cube(\"dept\",\"designation\").agg(sum(\"sales\")).orderBy(\"dept\",\"designation\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. A Dataframe contains sales of employees within the marketing dept. The team has to find out the lowest and highest sale made by all employees broken down by department. What command would satisfy below requirement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+\n",
      "|          dept|min(sales)|max(sales)|\n",
      "+--------------+----------+----------+\n",
      "|SALES - ENERGY|     18000|     25000|\n",
      "|SALES - RETAIL|     12000|     50000|\n",
      "+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"dept\").agg(min(\"sales\"),max(\"sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21.The Dataframe consists of records that have a values of \"Ad\" in the department column. For a departmental salary analysis, the records with \"Ad\" need to be substituted with \"Admin Dept\". Choose the appropriate command to achieve this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',\"Ad\",10000],\n",
    "\n",
    "                    ['Jill','Sales',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "|Tony|   HR|  8000|\n",
      "|Mona|Admin| 10000|\n",
      "|Jill|Sales|  5000|\n",
      "| Tim|Admin|  7000|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.replace(\"Ad\",\"Admin\",subset=['dept']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. There are multiple options to view names of columns once they have been defined in a schema structure. All options given below will work except one. Choose the invalid option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "SCHEMA = StructType([\n",
    "\n",
    "            StructField(\"name\",StringType(),True),\n",
    "\n",
    "            StructField(\"dept\",StringType(),True),\n",
    "\n",
    "            StructField(\"salary\",IntegerType(),True)\n",
    "\n",
    "])\n",
    "\n",
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,IntegerType,true)))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,IntegerType,true)))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfEmp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructType' object has no attribute 'columnNames'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-d31e8e753396>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSCHEMA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumnNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'StructType' object has no attribute 'columnNames'"
     ]
    }
   ],
   "source": [
    "SCHEMA.columnNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'dept', 'salary']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA.fieldNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. A schema structure has been created initially with a set of columns. Task is to modify the schema to create a new field. Choose the valid option below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = StructType([\n",
    "\n",
    "            StructField(\"name\",StringType(),True),\n",
    "\n",
    "            StructField(\"dept\",StringType(),True),\n",
    "\n",
    "            StructField(\"salary\",IntegerType(),True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructType' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-59cd15bf3a57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSCHEMA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStructType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"commision_percent\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIntegerType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'StructType' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "SCHEMA.append(StructType([StructField(\"commision_percent\",IntegerType(),True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,IntegerType,true),StructField(commision_percent,IntegerType,true)))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA.add(\"commision_percent\",IntegerType(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. From the below code, estimate the salary of Mona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testcomm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-db43649c1491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"testcomm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_comm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdfEmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestcomm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"salary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'testcomm' is not defined"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "def test_comm(x):\n",
    "\n",
    "  return x * 1.05\n",
    "\n",
    "spark.udf.register(\"testcomm\", test_comm)\n",
    "\n",
    "dfEmp.select(col(\"name\"),testcomm(col(\"salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. For the given code below, what is the \"salesDenseRank\" for \"Headphones\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|  itemName|sales_quantity|salesDenseRank|\n",
      "+----------+--------------+--------------+\n",
      "|        TV|           200|             1|\n",
      "|    Phones|           300|             2|\n",
      "|    Office|           300|             2|\n",
      "|Headphones|           400|             3|\n",
      "|   Kitchen|           500|             4|\n",
      "+----------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "dfSales= spark.createDataFrame([\n",
    "\n",
    "                    ['TV',200],\n",
    "\n",
    "                    ['Headphones',400],\n",
    "\n",
    "                    ['Phones',300],\n",
    "\n",
    "                    ['Kitchen',500],\n",
    "\n",
    "                    ['Office',300]],('itemName','sales_quantity'))\n",
    "\n",
    "windowSpec = Window.orderBy(\"sales_quantity\")\n",
    "\n",
    "salesDenseRank = dense_rank().over(windowSpec)\n",
    "\n",
    "dfSales.select(\n",
    "\n",
    "       col(\"itemName\"),\n",
    "\n",
    "       col(\"sales_quantity\"),\n",
    "\n",
    "       salesDenseRank.alias(\"salesDenseRank\")\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. Choose the correct option for row \"William T\", based on code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+\n",
      "| dept|salary|explodeName|\n",
      "+-----+------+-----------+\n",
      "|   HR|  8000|    William|\n",
      "|   HR|  8000|          T|\n",
      "|   HR|  8000|        Bob|\n",
      "|   HR|  8000|          H|\n",
      "|   HR|  5000|       Lisa|\n",
      "|   HR|  5000|          M|\n",
      "|ADMIN|  7000|      Marie|\n",
      "|ADMIN|  7000|          A|\n",
      "+-----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "                    ['William T',\"HR\",8000],\n",
    "\n",
    "                    ['Bob H',\"HR\",8000],\n",
    "\n",
    "                    ['Lisa M','HR',5000],\n",
    "\n",
    "                    ['Marie A','ADMIN',7000]],('name','dept','salary'))\n",
    "\n",
    "df = df.withColumn(\"splitName\", split(col(\"name\"), \" \"))\n",
    "\n",
    "df = df.withColumn(\"explodeName\", explode(col(\"splitName\")))\n",
    "\n",
    "df.select(\"dept\", \"salary\", \"explodeName\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. Choose the correct option below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         keys|\n",
      "+-------------+\n",
      "|[John, Marie]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(create_map(lit(\"1\"), lit(\"John\"),lit(\"2\"),lit(\"Marie\")).alias(\"empDetails\")).limit(1)\n",
    "\n",
    "df.select(map_values(\"empDetails\").alias(\"keys\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
