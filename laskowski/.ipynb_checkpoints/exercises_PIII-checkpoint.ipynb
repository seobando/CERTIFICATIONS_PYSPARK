{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Apache Sparkâ„¢ and Scala Workshops PART - III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are my own solutions version in PySpark of the Exercises proposed by Jacek Laskowski in https://github.com/jaceklaskowski/spark-workshop/tree/gh-pages/exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "21. [Using pivot to generate a single-row matrix](#21) \n",
    "22. [Using pivot for Cost Average and Collecting Values](#22)\n",
    "23. [Pivoting on Multiple Columns](#23)\n",
    "24. [Generating Exam Assessment Report](#24) \n",
    "25. [Flattening Dataset from Long to Wide Format](#25)\n",
    "26. [Finding 1st and 2nd Bestsellers Per Genre](#26)\n",
    "27. [Calculating aggregations](#27) \n",
    "28. [Calculating Running Total / Cumulative Sum](#28)\n",
    "29. [Calculating Difference Between Consecutive Rows Per Window](#29)\n",
    "30. [Converting Arrays of Strings to String](#30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sobando\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EN2010333.endava.net:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1bc642f73a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Using pivot to generate a single-row matrix <a id='21'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  update|   cc|\n",
      "+--------+-----+\n",
      "|20090622|  458|\n",
      "|20090624|31068|\n",
      "|20090626|  151|\n",
      "|20090629|  148|\n",
      "|20090914|  453|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "[20090622,458],\n",
    "[20090624,31068],\n",
    "[20090626,151],\n",
    "[20090629,148],\n",
    "[20090914,453]],(\"update\",\"cc\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+--------+--------+--------+\n",
      "|update|20090622|20090624|20090626|20090629|20090914|\n",
      "+------+--------+--------+--------+--------+--------+\n",
      "|    cc|     458|   31068|     151|     148|     453|\n",
      "+------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set a constant to groupby\n",
    "df_output = df_input.withColumn(\"constant\",lit(1))\n",
    "# Pivot the table to transpose it\n",
    "df_output = (df_output\n",
    "             .groupby(\"constant\")\n",
    "             .pivot(\"update\")\n",
    "             .agg(min(\"cc\")))\n",
    "# Renamed\n",
    "df_output = (df_output\n",
    "             .withColumn(\"constant\",lit(\"cc\"))\n",
    "             .withColumnRenamed(\"constant\",\"update\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Using pivot for Cost Average and Collecting Values <a id='22'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+----+\n",
      "| id|type|cost|  date|ship|\n",
      "+---+----+----+------+----+\n",
      "|  0|   A| 223|201603|PORT|\n",
      "|  0|   A|  22|201602|PORT|\n",
      "|  0|   A| 422|201601|DOCK|\n",
      "|  1|   B|3213|201602|DOCK|\n",
      "|  1|   B|3213|201601|PORT|\n",
      "|  2|   C|2321|201601|DOCK|\n",
      "+---+----+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "  [0, \"A\", 223, \"201603\", \"PORT\"],\n",
    "  [0, \"A\", 22, \"201602\", \"PORT\"],\n",
    "  [0, \"A\", 422, \"201601\", \"DOCK\"],\n",
    "  [1, \"B\", 3213, \"201602\", \"DOCK\"],\n",
    "  [1, \"B\", 3213, \"201601\", \"PORT\"],\n",
    "  [2, \"C\", 2321, \"201601\", \"DOCK\"]],(\"id\",\"type\", \"cost\", \"date\", \"ship\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------+------+\n",
      "| id|type|201601|201602|201603|\n",
      "+---+----+------+------+------+\n",
      "|  0|   A| 422.0|  22.0| 223.0|\n",
      "|  1|   B|3213.0|3213.0|  null|\n",
      "|  2|   C|2321.0|  null|  null|\n",
      "+---+----+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result I\n",
    "df_output = (df_input\n",
    "             .groupby(\"id\",\"type\")\n",
    "             .pivot(\"date\")\n",
    "             .agg(avg(\"cost\"))\n",
    "             .orderBy(\"id\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------+------+\n",
      "| id|type|201601|201602|201603|\n",
      "+---+----+------+------+------+\n",
      "|  0|   A|[DOCK]|[PORT]|[PORT]|\n",
      "|  1|   B|[PORT]|[DOCK]|    []|\n",
      "|  2|   C|[DOCK]|    []|    []|\n",
      "+---+----+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result II\n",
    "df_output = (df_input\n",
    "             .groupby(\"id\",\"type\")\n",
    "             .pivot(\"date\")\n",
    "             .agg(collect_set(\"ship\"))\n",
    "             .orderBy(\"id\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Pivoting on Multiple Columns <a id='23'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "| id|day|price|units|\n",
      "+---+---+-----+-----+\n",
      "|100|  1|   23|   10|\n",
      "|100|  2|   45|   11|\n",
      "|100|  3|   67|   12|\n",
      "|100|  4|   78|   13|\n",
      "|101|  1|   23|   10|\n",
      "|101|  2|   45|   13|\n",
      "|101|  3|   67|   14|\n",
      "|101|  4|   78|   15|\n",
      "|102|  1|   23|   10|\n",
      "|102|  2|   45|   11|\n",
      "|102|  3|   67|   16|\n",
      "|102|  4|   78|   18|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "  [100,1,23,10],\n",
    "  [100,2,45,11],\n",
    "  [100,3,67,12],\n",
    "  [100,4,78,13],\n",
    "  [101,1,23,10],\n",
    "  [101,2,45,13],\n",
    "  [101,3,67,14],\n",
    "  [101,4,78,15],\n",
    "  [102,1,23,10],\n",
    "  [102,2,45,11],\n",
    "  [102,3,67,16],\n",
    "  [102,4,78,18]],(\"id\", \"day\", \"price\", \"units\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------+------+-------+------+-------+------+\n",
      "| id|1_price|1_unit|2_price|2_unit|3_price|3_unit|4_price|4_unit|\n",
      "+---+-------+------+-------+------+-------+------+-------+------+\n",
      "|100|     23|    10|     45|    11|     67|    12|     78|    13|\n",
      "|101|     23|    10|     45|    13|     67|    14|     78|    15|\n",
      "|102|     23|    10|     45|    11|     67|    16|     78|    18|\n",
      "+---+-------+------+-------+------+-------+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/45035940/how-to-pivot-on-multiple-columns-in-spark-sql\n",
    "df_output = (df_input\n",
    "             .groupBy('id')\n",
    "             .pivot('day')\n",
    "             .agg(first('price').alias('price'),first('units').alias('unit')))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. Generating Exam Assessment Report <a id='24'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+-------------+----------+-------+\n",
      "|Qid|     Question|AnswerText|ParticipantID|Assessment| GeoTag|\n",
      "+---+-------------+----------+-------------+----------+-------+\n",
      "|  1|Question1Text|       Yes|       abcde1|         0|(x1,y1)|\n",
      "|  2|Question2Text|        No|       abcde1|         0|(x1,y1)|\n",
      "|  3|Question3Text|         3|       abcde1|         0|(x1,y1)|\n",
      "|  1|Question1Text|        No|       abcde2|         0|(x2,y2)|\n",
      "|  2|Question2Text|       Yes|       abcde2|         0|(x2,y2)|\n",
      "+---+-------------+----------+-------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,\"Question1Text\",\"Yes\",\"abcde1\",0,\"(x1,y1)\"],\n",
    "    [2,\"Question2Text\",\"No\",\"abcde1\",0,\"(x1,y1)\"],\n",
    "    [3,\"Question3Text\",\"3\",\"abcde1\",0,\"(x1,y1)\"],\n",
    "    [1,\"Question1Text\",\"No\",\"abcde2\",0,\"(x2,y2)\"],\n",
    "    [2,\"Question2Text\",\"Yes\",\"abcde2\",0,\"(x2,y2)\"]],(\"Qid\",\"Question\",\"AnswerText\",\"ParticipantID\",\"Assessment\",\"GeoTag\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------+-----+-----+-----+\n",
      "|ParticipantID|Assessment| GeoTag|Qid_1|Qid_2|Qid_3|\n",
      "+-------------+----------+-------+-----+-----+-----+\n",
      "|       abcde2|         0|(x2,y2)|   No|  Yes| null|\n",
      "|       abcde1|         0|(x1,y1)|  Yes|   No|    3|\n",
      "+-------------+----------+-------+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/47720822/how-to-pivot-on-arbitrary-column\n",
    "df_output = df_input.withColumn(\"header\", concat(lit(\"Qid_\"), \"Qid\"))\n",
    "df_output = (df_output\n",
    "             .groupBy('ParticipantID', 'Assessment', 'GeoTag')\n",
    "             .pivot('header')\n",
    "             .agg(first('AnswerText')))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Flattening Dataset from Long to Wide Format <a id='25'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+\n",
      "|key|val1|val2|date|\n",
      "+---+----+----+----+\n",
      "| k1|  v4|  v7|  d1|\n",
      "| k1|  v5|  v8|  d2|\n",
      "| k1|  v6|  v9|  d3|\n",
      "| k2| v12| v22|  d1|\n",
      "| k2| v32| v42|  d2|\n",
      "| k2| v11| v21|  d3|\n",
      "+---+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "[\"k1\",\"v4\",\"v7\",\"d1\"],\n",
    "[\"k1\",\"v5\",\"v8\",\"d2\"],\n",
    "[\"k1\",\"v6\",\"v9\",\"d3\"],\n",
    "[\"k2\",\"v12\",\"v22\",\"d1\"],\n",
    "[\"k2\",\"v32\",\"v42\",\"d2\"],\n",
    "[\"k2\",\"v11\",\"v21\",\"d3\"]],(\"key\",\"val1\",\"val2\",\"date\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+\n",
      "|key|d1_v1|d1_v2|d2_v1|d2_v2|d3_v1|d3_v2|\n",
      "+---+-----+-----+-----+-----+-----+-----+\n",
      "| k2|  v12|  v22|  v32|  v42|  v11|  v21|\n",
      "| k1|   v4|   v7|   v5|   v8|   v6|   v9|\n",
      "+---+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/43349932/how-to-flatten-long-dataset-to-wide-format-pivot-with-no-join\n",
    "df_output = (df_input\n",
    "             .groupby(\"key\")\n",
    "             .pivot('date')\n",
    "             .agg(first('val1').alias('v1'),first('val2').alias('v2')))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. Finding 1st and 2nd Bestsellers Per Genre <a id='26'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------+--------+\n",
      "| id|            title|   genre|quantity|\n",
      "+---+-----------------+--------+--------+\n",
      "|  1|    Hunter Fields| romance|      15|\n",
      "|  2|    Leonard Lewis|thriller|      81|\n",
      "|  3|     Jason Dawson|thriller|      90|\n",
      "|  4|      Andre Grant|thriller|      25|\n",
      "|  5|      Earl Walton| romance|      40|\n",
      "|  6|      Alan Hanson| romance|      24|\n",
      "|  7|   Clyde Matthews|thriller|      31|\n",
      "|  8|Josephine Leonard|thriller|       1|\n",
      "|  9|       Owen Boone|  sci-fi|      27|\n",
      "| 10|      Max McBride| romance|      75|\n",
      "+---+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,\"Hunter Fields\",\"romance\",15],\n",
    "    [2,\"Leonard Lewis\",\"thriller\",81],\n",
    "    [3,\"Jason Dawson\",\"thriller\",90],\n",
    "    [4,\"Andre Grant\",\"thriller\",25],\n",
    "    [5,\"Earl Walton\",\"romance\",40],\n",
    "    [6,\"Alan Hanson\",\"romance\",24],\n",
    "    [7,\"Clyde Matthews\",\"thriller\",31],\n",
    "    [8,\"Josephine Leonard\",\"thriller\",1],\n",
    "    [9,\"Owen Boone\",\"sci-fi\",27],\n",
    "    [10,\"Max McBride\",\"romance\",75]], (\"id\",\"title\",\"genre\",\"quantity\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------+--------+----+\n",
      "| id|        title|   genre|quantity|rank|\n",
      "+---+-------------+--------+--------+----+\n",
      "| 10|  Max McBride| romance|      75|   1|\n",
      "|  5|  Earl Walton| romance|      40|   2|\n",
      "|  3| Jason Dawson|thriller|      90|   1|\n",
      "|  2|Leonard Lewis|thriller|      81|   2|\n",
      "|  9|   Owen Boone|  sci-fi|      27|   1|\n",
      "+---+-------------+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = df_input.withColumn(\"rank\", rank().over(Window.partitionBy(\"genre\").orderBy(desc(\"quantity\"))))\n",
    "df_output = df_output.filter(\"rank <= 2\")\n",
    "df_output.show()                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. Calculating Gap Between Current And Highest Salaries Per Department <a id='27'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+------+\n",
      "| id|             name|department|salary|\n",
      "+---+-----------------+----------+------+\n",
      "|  1|    Hunter Fields|        IT|    15|\n",
      "|  2|    Leonard Lewis|   Support|    81|\n",
      "|  3|     Jason Dawson|   Support|    90|\n",
      "|  4|      Andre Grant|   Support|    25|\n",
      "|  5|      Earl Walton|        IT|    40|\n",
      "|  6|      Alan Hanson|        IT|    24|\n",
      "|  7|   Clyde Matthews|   Support|    31|\n",
      "|  8|Josephine Leonard|   Support|     1|\n",
      "|  9|       Owen Boone|        HR|    27|\n",
      "| 10|      Max McBride|        IT|    75|\n",
      "+---+-----------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,\"Hunter Fields\",\"IT\",15],\n",
    "    [2,\"Leonard Lewis\",\"Support\",81],\n",
    "    [3,\"Jason Dawson\",\"Support\",90],\n",
    "    [4,\"Andre Grant\",\"Support\",25],\n",
    "    [5,\"Earl Walton\",\"IT\",40],\n",
    "    [6,\"Alan Hanson\",\"IT\",24],\n",
    "    [7,\"Clyde Matthews\",\"Support\",31],\n",
    "    [8,\"Josephine Leonard\",\"Support\",1],\n",
    "    [9,\"Owen Boone\",\"HR\",27],\n",
    "    [10,\"Max McBride\",\"IT\",75]],(\"id\",\"name\",\"department\",\"salary\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+------+----+\n",
      "| id|             name|department|salary|diff|\n",
      "+---+-----------------+----------+------+----+\n",
      "|  9|       Owen Boone|        HR|    27|   0|\n",
      "|  1|    Hunter Fields|        IT|    15|  60|\n",
      "|  5|      Earl Walton|        IT|    40|  35|\n",
      "|  6|      Alan Hanson|        IT|    24|  51|\n",
      "| 10|      Max McBride|        IT|    75|   0|\n",
      "|  2|    Leonard Lewis|   Support|    81|   9|\n",
      "|  3|     Jason Dawson|   Support|    90|   0|\n",
      "|  4|      Andre Grant|   Support|    25|  65|\n",
      "|  7|   Clyde Matthews|   Support|    31|  59|\n",
      "|  8|Josephine Leonard|   Support|     1|  89|\n",
      "+---+-----------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get max salary by department\n",
    "df_max_salary = df_input.groupby(\"department\").agg(max(\"salary\"))\n",
    "# Avoid department repetition when join\n",
    "df_max_salary = df_max_salary.withColumnRenamed(\"department\",\"max_department\")\n",
    "# Join\n",
    "df_output = df_input.join(df_max_salary,df_input[\"department\"] == df_max_salary[\"max_department\"],\"left\")\n",
    "# Drop repeated deparment\n",
    "df_output = df_output.drop(\"max_department\")\n",
    "# Calculate difference\n",
    "df_output = df_output.withColumn(\"diff\",col(\"max(salary)\")-col(\"salary\")).drop(\"max(salary)\")\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28. Calculating Running Total / Cumulative Sum <a id='28'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n",
      "|time|department|items_sold|\n",
      "+----+----------+----------+\n",
      "|   1|        IT|        15|\n",
      "|   2|   Support|        81|\n",
      "|   3|   Support|        90|\n",
      "|   4|   Support|        25|\n",
      "|   5|        IT|        40|\n",
      "|   6|        IT|        24|\n",
      "|   7|   Support|        31|\n",
      "|   8|   Support|         1|\n",
      "|   9|        HR|        27|\n",
      "|  10|        IT|        75|\n",
      "+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,\"IT\",15],\n",
    "    [2,\"Support\",81],\n",
    "    [3,\"Support\",90],\n",
    "    [4,\"Support\",25],\n",
    "    [5,\"IT\",40],\n",
    "    [6,\"IT\",24],\n",
    "    [7,\"Support\",31],\n",
    "    [8,\"Support\",1],\n",
    "    [9,\"HR\",27],\n",
    "    [10,\"IT\",75]],(\"time\",\"department\",\"items_sold\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------+\n",
      "|time|department|items_sold|running_total|\n",
      "+----+----------+----------+-------------+\n",
      "|   9|        HR|        27|           27|\n",
      "|   1|        IT|        15|           15|\n",
      "|   5|        IT|        40|           55|\n",
      "|   6|        IT|        24|           79|\n",
      "|  10|        IT|        75|          154|\n",
      "|   2|   Support|        81|           81|\n",
      "|   3|   Support|        90|          171|\n",
      "|   4|   Support|        25|          196|\n",
      "|   7|   Support|        31|          227|\n",
      "|   8|   Support|         1|          228|\n",
      "+----+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowval = (Window.partitionBy('department').orderBy('time')\n",
    "             .rangeBetween(Window.unboundedPreceding, 0))\n",
    "df_output = df_input.withColumn('running_total', sum('items_sold').over(windowval))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29. Calculating Difference Between Consecutive Rows Per Window <a id='29'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------+\n",
      "|time|department|items_sold|running_total|\n",
      "+----+----------+----------+-------------+\n",
      "|   1|        IT|        15|           15|\n",
      "|   2|   Support|        81|           81|\n",
      "|   3|   Support|        90|          171|\n",
      "|   4|   Support|        25|          196|\n",
      "|   5|        IT|        40|           55|\n",
      "|   6|        IT|        24|           79|\n",
      "|   7|   Support|        31|          227|\n",
      "|   8|   Support|         1|          228|\n",
      "|   9|        HR|        27|           27|\n",
      "|  10|        IT|        75|          154|\n",
      "+----+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,\"IT\",15,15],\n",
    "    [2,\"Support\",81,81],\n",
    "    [3,\"Support\",90,171],\n",
    "    [4,\"Support\",25,196],\n",
    "    [5,\"IT\",40,55],\n",
    "    [6,\"IT\",24,79],\n",
    "    [7,\"Support\",31,227],\n",
    "    [8,\"Support\",1,228],\n",
    "    [9,\"HR\",27,27],\n",
    "    [10,\"IT\",75,154]],(\"time\",\"department\",\"items_sold\",\"running_total\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------+\n",
      "|time|department|items_sold|running_total|\n",
      "+----+----------+----------+-------------+\n",
      "|   9|        HR|        27|           27|\n",
      "|   1|        IT|        15|           15|\n",
      "|   5|        IT|        40|           55|\n",
      "|   6|        IT|        24|           79|\n",
      "|  10|        IT|        75|          154|\n",
      "|   2|   Support|        81|           81|\n",
      "|   3|   Support|        90|          171|\n",
      "|   4|   Support|        25|          196|\n",
      "|   7|   Support|        31|          227|\n",
      "|   8|   Support|         1|          228|\n",
      "+----+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimate window by department and order by time\n",
    "windowval = (Window.partitionBy('department').orderBy('time')\n",
    "             .rangeBetween(Window.unboundedPreceding, 0))\n",
    "# Apply cummulative sum\n",
    "df_output = df_input.withColumn('running_total', sum('items_sold').over(windowval))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------+----+\n",
      "|time|department|items_sold|running_total|diff|\n",
      "+----+----------+----------+-------------+----+\n",
      "|   9|        HR|        27|           27|  27|\n",
      "|   1|        IT|        15|           15|  15|\n",
      "|   5|        IT|        40|           55|  40|\n",
      "|   6|        IT|        24|           79|  24|\n",
      "|  10|        IT|        75|          154|  75|\n",
      "|   2|   Support|        81|           81|  81|\n",
      "|   3|   Support|        90|          171|  90|\n",
      "|   4|   Support|        25|          196|  25|\n",
      "|   7|   Support|        31|          227|  31|\n",
      "|   8|   Support|         1|          228|   1|\n",
      "+----+----------+----------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get lag value by department\n",
    "df_output = df_output.withColumn(\"lag\",lag(col(\"running_total\")).over(Window.partitionBy(\"department\").orderBy(\"time\")))\n",
    "# Fill null values of lag values without previous department\n",
    "df_output = df_output.withColumn(\"lag\",when(col(\"lag\").isNull(),0).otherwise(col(\"lag\")))\n",
    "# Get difference\n",
    "df_output = df_output.withColumn(\"diff\",col(\"running_total\")-col(\"lag\")).drop(\"lag\")\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. Converting Arrays of Strings to String <a id='30'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|         words|\n",
      "+---+--------------+\n",
      "|  1|[hello, world]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,[\"hello\", \"world\"]]],(\"id\",\"words\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|         words|   solution|\n",
      "+--------------+-----------+\n",
      "|[hello, world]|hello world|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Array elements\n",
    "df_output = (df_input\n",
    "             .withColumn(\"element_1\",element_at(\"words\",1))\n",
    "             .withColumn(\"element_2\",element_at(\"words\",2)))\n",
    "# Get solution\n",
    "df_output = (df_output\n",
    "             .withColumn(\"solution\",concat_ws(\" \",col(\"element_1\"),col(\"element_2\")))\n",
    "             .drop(\"id\",\"element_1\",\"element_2\"))\n",
    "df_output.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
