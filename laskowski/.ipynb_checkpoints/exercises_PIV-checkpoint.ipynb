{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Apache Sparkâ„¢ and Scala Workshops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are my own solutions version in PySpark of the Exercises proposed by Jacek Laskowski in https://github.com/jaceklaskowski/spark-workshop/tree/gh-pages/exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "31. [Calculating percent rank](#31) \n",
    "32. [Finding First Non-Null Value per Group](#32)\n",
    "33. [Finding Longest Sequence (Window Aggregation)](#33)\n",
    "34. [Finding Most Common Non-null Prefix per Group (Occurences)](#34) \n",
    "35. [Using rollup Operator for Total and Average Salaries by Department and Company-Wide](#35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sobando\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EN2010333.endava.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x210c9966400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. Calculating percent rank <a id='31'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Employee|Salary|\n",
      "+--------+------+\n",
      "|    Tony|    50|\n",
      "|    Alan|    45|\n",
      "|     Lee|    60|\n",
      "|   David|    35|\n",
      "|   Steve|    65|\n",
      "|    Paul|    48|\n",
      "|   Micky|    62|\n",
      "|  George|    80|\n",
      "|   Nigel|    64|\n",
      "|    John|    42|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [\"Tony\",50],\n",
    "    [\"Alan\",45],\n",
    "    [\"Lee\",60],\n",
    "    [\"David\",35],\n",
    "    [\"Steve\",65],\n",
    "    [\"Paul\",48],\n",
    "    [\"Micky\",62],\n",
    "    [\"George\",80],\n",
    "    [\"Nigel\",64],\n",
    "    [\"John\",42]],(\"Employee\",\"Salary\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+\n",
      "|Employee|Salary|Percentage|\n",
      "+--------+------+----------+\n",
      "|  George|    80|      High|\n",
      "|   Steve|    65|      High|\n",
      "|   Nigel|    64|      High|\n",
      "|   Micky|    62|   Average|\n",
      "|     Lee|    60|   Average|\n",
      "|    Tony|    50|   Average|\n",
      "|    Paul|    48|   Average|\n",
      "|    Alan|    45|       Low|\n",
      "|    John|    42|       Low|\n",
      "|   David|    35|       Low|\n",
      "+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = (df_input\n",
    "             .withColumn(\"Percentage\",\n",
    "                         percent_rank()\n",
    "                         .over(Window.orderBy(\"Salary\")))\n",
    "             .withColumn(\"Percentage\"\n",
    "                         ,when(col(\"Percentage\") > 0.7, \"High\" )\n",
    "                         .when(col(\"Percentage\") > 0.3,\"Average\")\n",
    "                         .otherwise(\"Low\"))\n",
    "             .orderBy(col(\"Salary\").desc()))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32. Finding First Non-Null Value per Group <a id='32'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  id|group|\n",
      "+----+-----+\n",
      "|null|    0|\n",
      "|null|    1|\n",
      "|   2|    0|\n",
      "|null|    1|\n",
      "|   4|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "  [None, 0],\n",
    "  [None, 1],\n",
    "  [2, 0],\n",
    "  [None, 1],\n",
    "  [4, 1]],(\"id\", \"group\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------+\n",
      "|  id|group|first_non_null|\n",
      "+----+-----+--------------+\n",
      "|null|    0|             2|\n",
      "|   2|    0|             2|\n",
      "|null|    1|             4|\n",
      "|null|    1|             4|\n",
      "|   4|    1|             4|\n",
      "+----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set Window\n",
    "firstWindow = (Window\n",
    "               .partitionBy(\"group\")\n",
    "               .orderBy(\"id\")\n",
    "               .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing))\n",
    "# Get first Non Null Values\n",
    "df_output = df_input.withColumn(\"first_non_null\",first(\"id\",ignorenulls=True).over(firstWindow))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|group|first_non_null|\n",
      "+-----+--------------+\n",
      "|    0|             2|\n",
      "|    1|             4|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df_output = df_output.select(\"group\",\"first_non_null\").dropDuplicates()\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33. Finding Longest Sequence (Window Aggregation) <a id='33'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| ID|time|\n",
      "+---+----+\n",
      "|  1|   1|\n",
      "|  1|   2|\n",
      "|  1|   4|\n",
      "|  1|   7|\n",
      "|  1|   8|\n",
      "|  1|   9|\n",
      "|  2|   1|\n",
      "|  3|   1|\n",
      "|  3|   2|\n",
      "|  3|   3|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,1],\n",
    "    [1,2],\n",
    "    [1,4],\n",
    "    [1,7],\n",
    "    [1,8],\n",
    "    [1,9],\n",
    "    [2,1],\n",
    "    [3,1],\n",
    "    [3,2],\n",
    "    [3,3]],(\"ID\",\"time\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34. Finding Most Common Non-null Prefix per Group (Occurences) <a id='34'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|UNIQUE_GUEST_ID|PREFIX|\n",
      "+---------------+------+\n",
      "|              1|    Mr|\n",
      "|              1|   Mme|\n",
      "|              1|    Mr|\n",
      "|              1|  null|\n",
      "|              1|  null|\n",
      "|              1|  null|\n",
      "|              2|    Mr|\n",
      "|              3|  null|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "  [1, \"Mr\"],\n",
    "  [1, \"Mme\"],\n",
    "  [1, \"Mr\"],\n",
    "  [1, None],\n",
    "  [1, None],\n",
    "  [1, None],\n",
    "  [2, \"Mr\"],\n",
    "  [3, None]],(\"UNIQUE_GUEST_ID\", \"PREFIX\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+-----------+\n",
      "|UNIQUE_GUEST_ID|PREFIX|Repetitions|\n",
      "+---------------+------+-----------+\n",
      "|              1|    Mr|          2|\n",
      "|              3|  null|          0|\n",
      "|              2|    Mr|          1|\n",
      "+---------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set row Window\n",
    "rowWindow = (Window\n",
    "               .partitionBy(\"UNIQUE_GUEST_ID\")\n",
    "               .orderBy(col(\"Repetitions\").desc()))\n",
    "# Estimate repetitions               \n",
    "df_output = (df_input\n",
    "             .groupby(\"UNIQUE_GUEST_ID\",\"PREFIX\")\n",
    "             .agg(count(\"PREFIX\").alias(\"Repetitions\"))\n",
    "             .orderBy(\"UNIQUE_GUEST_ID\",col(\"Repetitions\").desc())\n",
    "            )\n",
    "# Get first row\n",
    "df_output = (df_output\n",
    "             .withColumn(\"rn\",row_number().over(rowWindow))\n",
    "             .where(col(\"rn\") == 1)\n",
    "             .drop(\"rn\")\n",
    "             .orderBy(\"UNIQUE_GUEST_ID\")\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35. Using rollup Operator for Total and Average Salaries by Department and Company-Wide <a id='35'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+------+\n",
      "| id|             name|department|salary|\n",
      "+---+-----------------+----------+------+\n",
      "|  1|    Hunter Fields|        IT|    15|\n",
      "|  2|    Leonard Lewis|   Support|    81|\n",
      "|  3|     Jason Dawson|   Support|    90|\n",
      "|  4|      Andre Grant|   Support|    25|\n",
      "|  5|      Earl Walton|        IT|    40|\n",
      "|  6|      Alan Hanson|        IT|    24|\n",
      "|  7|   Clyde Matthews|   Support|    31|\n",
      "|  8|Josephine Leonard|   Support|     1|\n",
      "|  9|       Owen Boone|        HR|    27|\n",
      "| 10|      Max McBride|        IT|    75|\n",
      "+---+-----------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,\"Hunter Fields\",\"IT\",15],\n",
    "    [2,\"Leonard Lewis\",\"Support\",81],\n",
    "    [3,\"Jason Dawson\",\"Support\",90],\n",
    "    [4,\"Andre Grant\",\"Support\",25],\n",
    "    [5,\"Earl Walton\",\"IT\",40],\n",
    "    [6,\"Alan Hanson\",\"IT\",24],\n",
    "    [7,\"Clyde Matthews\",\"Support\",31],\n",
    "    [8,\"Josephine Leonard\",\"Support\",1],\n",
    "    [9,\"Owen Boone\",\"HR\",27],\n",
    "    [10,\"Max McBride\",\"IT\",75]], (\"id\",\"name\",\"department\",\"salary\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+\n",
      "|department|sum(salary)|avg(salary)|\n",
      "+----------+-----------+-----------+\n",
      "|      null|        409|       40.9|\n",
      "|   Support|        228|       45.6|\n",
      "|        IT|        154|       38.5|\n",
      "|        HR|         27|       27.0|\n",
      "+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = (df_input\n",
    "             .rollup(\"department\")\n",
    "             .agg(sum(\"salary\"),avg(\"salary\")))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|      null|         90|\n",
      "|   Support|         90|\n",
      "|        IT|         75|\n",
      "|        HR|         27|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rollup enables to apply aggregation function over specific groups\n",
    "df_input.rollup(\"department\").agg(max(\"salary\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
