{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Apache Sparkâ„¢ and Scala Workshops PART - II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are my own solutions version in PySpark of the Exercises proposed by Jacek Laskowski in https://github.com/jaceklaskowski/spark-workshop/tree/gh-pages/exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "11. [Finding Most Populated Cities Per Country](#11) \n",
    "12. [Using upper Standard Function](#12)\n",
    "13. [Difference in Days Between Dates As Strings](#13)\n",
    "14. [Counting Occurences Of Years and Months For 24 Months From Now](#14) No es claro\n",
    "15. [Why are all fields null when querying with schema?](#15) No es claro\n",
    "16. [How to add days (as values of a column) to date?](#16)\n",
    "17. [Calculating aggregations](#17) \n",
    "18. [Finding maximum values per group (groupBy)](#18)\n",
    "19. [Collect values per group](#19)\n",
    "20. [Multiple Aggregations](#20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sobando\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EN2010333.endava.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x13ec81af4c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Finding Most Populated Cities Per Country <a id='11'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+----------+\n",
      "|             name|      country|population|\n",
      "+-----------------+-------------+----------+\n",
      "|           Warsaw|       Poland| 1 764 615|\n",
      "|           Cracow|       Poland|   769 498|\n",
      "|            Paris|       France| 2 206 488|\n",
      "|Villeneuve-Loubet|       France|    15 020|\n",
      "|    Pittsburgh PA|United States|   302 407|\n",
      "|       Chicago IL|United States| 2 716 000|\n",
      "|     Milwaukee WI|United States|   595 351|\n",
      "|          Vilnius|    Lithuania|   580 020|\n",
      "|        Stockholm|       Sweden|   972 647|\n",
      "|         Goteborg|       Sweden|   580 020|\n",
      "+-----------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [\"Warsaw\",\"Poland\",\"1 764 615\"],\n",
    "    [\"Cracow\",\"Poland\",\"769 498\"],\n",
    "    [\"Paris\",\"France\",\"2 206 488\"],\n",
    "    [\"Villeneuve-Loubet\",\"France\",\"15 020\"],\n",
    "    [\"Pittsburgh PA\",\"United States\",\"302 407\"],\n",
    "    [\"Chicago IL\",\"United States\",\"2 716 000\"],\n",
    "    [\"Milwaukee WI\",\"United States\",\"595 351\"],\n",
    "    [\"Vilnius\",\"Lithuania\",\"580 020\"],\n",
    "    [\"Stockholm\",\"Sweden\",\"972 647\"],\n",
    "    [\"Goteborg\",\"Sweden\",\"580 020\"]\n",
    "],(\"name\",\"country\",\"population\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+----------+\n",
      "|             name|      country|population|\n",
      "+-----------------+-------------+----------+\n",
      "|       Chicago IL|United States| 2 716 000|\n",
      "|            Paris|       France| 2 206 488|\n",
      "|           Warsaw|       Poland| 1 764 615|\n",
      "|        Stockholm|       Sweden|   972 647|\n",
      "|           Cracow|       Poland|   769 498|\n",
      "|     Milwaukee WI|United States|   595 351|\n",
      "|         Goteborg|       Sweden|   580 020|\n",
      "|          Vilnius|    Lithuania|   580 020|\n",
      "|    Pittsburgh PA|United States|   302 407|\n",
      "|Villeneuve-Loubet|       France|    15 020|\n",
      "+-----------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = df_input.withColumn(\"population_int\",regexp_replace(col(\"population\"), \" \", \"\").cast(IntegerType()))\n",
    "df_output = df_output.orderBy(col(\"population_int\").desc()).drop(\"population_int\")\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Using upper Standard Function <a id='12'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-------+\n",
      "| id|             city|country|\n",
      "+---+-----------------+-------+\n",
      "|  0|           Warsaw| Poland|\n",
      "|  1|Villeneuve-Loubet| France|\n",
      "|  2|           Vranje| Serbia|\n",
      "|  3|       Pittsburgh|     US|\n",
      "+---+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "[0,\"Warsaw\",\"Poland\"],\n",
    "[1,\"Villeneuve-Loubet\",\"France\"],\n",
    "[2,\"Vranje\",\"Serbia\"],\n",
    "[3,\"Pittsburgh\",\"US\"]],(\"id\",\"city\",\"country\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-------+-----------------+\n",
      "| id|             city|country|       upper_city|\n",
      "+---+-----------------+-------+-----------------+\n",
      "|  0|           Warsaw| Poland|           WARSAW|\n",
      "|  1|Villeneuve-Loubet| France|VILLENEUVE-LOUBET|\n",
      "|  2|           Vranje| Serbia|           VRANJE|\n",
      "|  3|       Pittsburgh|     US|       PITTSBURGH|\n",
      "+---+-----------------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = df_input.withColumn(\"upper_city\",upper(\"city\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Difference in Days Between Dates As Strings <a id='13'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|date_string|\n",
      "+---+-----------+\n",
      "|  1| 08/11/2015|\n",
      "|  2| 09/11/2015|\n",
      "|  3| 09/12/2015|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "   [1,\"08/11/2015\"],\n",
    "   [2,\"09/11/2015\"],\n",
    "   [3,\"09/12/2015\"]],(\"id\",\"date_string\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+----+\n",
      "| id|date_string|   to_date|diff|\n",
      "+---+-----------+----------+----+\n",
      "|  1| 08/11/2015|2015-11-08|1889|\n",
      "|  2| 09/11/2015|2015-11-09|1888|\n",
      "|  3| 09/12/2015|2015-12-09|1858|\n",
      "+---+-----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateFormat = \"dd/MM/yyyy\"\n",
    "df_output = df_input.withColumn(\"to_date\",to_date(col(\"date_string\"),dateFormat))\n",
    "df_output = df_output.withColumn(\"diff\",datediff(current_date(),col(\"to_date\")))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Counting Occurences Of Years and Months For 24 Months From Now <a id='14'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|YEAR_MONTH|AMOUNT|\n",
      "+----------+------+\n",
      "|    202001|   500|\n",
      "|    202001|   600|\n",
      "|    201912|   100|\n",
      "|    201910|   200|\n",
      "|    201910|   100|\n",
      "|    201909|   400|\n",
      "|    201601|  5000|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [202001,500],\n",
    "    [202001,600],\n",
    "    [201912,100],\n",
    "    [201910,200],\n",
    "    [201910,100],\n",
    "    [201909,400],\n",
    "    [201601,5000]],(\"YEAR_MONTH\",\"AMOUNT\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Why are all fields null when querying with schema? <a id='15'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-07-22 00:10:15,030|10.29.2.6|\n",
    "2019-07-22 00:10:15,334|10.1.198.41|\n",
    "2019-07-22 00:10:15,400|10.1.198.41|\n",
    "2019-07-22 00:10:15,511|10.1.198.41|\n",
    "2019-07-22 00:10:16,911|10.1.198.41|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. How to add days (as values of a column) to date? <a id='16'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|number_of_days|      date|\n",
      "+--------------+----------+\n",
      "|             0| 2016-01-1|\n",
      "|             1| 2016-02-2|\n",
      "|             2|2016-03-22|\n",
      "|             3|2016-04-25|\n",
      "|             4|2016-05-21|\n",
      "|             5| 2016-06-1|\n",
      "|             6|2016-03-21|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "  [0, \"2016-01-1\"],\n",
    "  [1, \"2016-02-2\"],\n",
    "  [2, \"2016-03-22\"],\n",
    "  [3, \"2016-04-25\"],\n",
    "  [4, \"2016-05-21\"],\n",
    "  [5, \"2016-06-1\"],\n",
    "  [6, \"2016-03-21\"]],(\"number_of_days\", \"date\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+----------+\n",
      "|number_of_days|      date|   to_date|    future|\n",
      "+--------------+----------+----------+----------+\n",
      "|             0| 2016-01-1|2016-01-01|2016-01-01|\n",
      "|             1| 2016-02-2|2016-02-02|2016-02-03|\n",
      "|             2|2016-03-22|2016-03-22|2016-03-24|\n",
      "|             3|2016-04-25|2016-04-25|2016-04-28|\n",
      "|             4|2016-05-21|2016-05-21|2016-05-25|\n",
      "|             5| 2016-06-1|2016-06-01|2016-06-06|\n",
      "|             6|2016-03-21|2016-03-21|2016-03-27|\n",
      "+--------------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set date format of the original string date\n",
    "dateFormat = \"yyyy-MM-d\"\n",
    "# Organize types\n",
    "df_output = (df_input\n",
    "             .withColumn(\"to_date\",to_date(col(\"date\"),dateFormat))\n",
    "             .withColumn(\"number_of_days\",col(\"number_of_days\").cast(IntegerType())))\n",
    "# Add days\n",
    "df_output = df_output.withColumn(\"future\", expr(\"date_add(to_date,number_of_days)\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Calculating aggregations <a id='17'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+\n",
      "| id|             name|population|\n",
      "+---+-----------------+----------+\n",
      "|  0|           Warsaw| 1 764 615|\n",
      "|  1|Villeneuve-Loubet|    15 020|\n",
      "|  2|           Vranje|    83 524|\n",
      "|  3|       Pittsburgh| 1 775 634|\n",
      "+---+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [0,'Warsaw','1 764 615'],\n",
    "    [1,'Villeneuve-Loubet','15 020'],\n",
    "    [2,'Vranje','83 524'],\n",
    "    [3,'Pittsburgh','1 775 634']],(\"id\",\"name\",\"population\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|population|\n",
      "+----------+\n",
      "|   1775634|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = df_input.withColumn(\"population_int\",regexp_replace(col(\"population\"), \" \", \"\").cast(IntegerType()))\n",
    "df_output = df_output.agg(max(\"population_int\").alias(\"population\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Finding maximum values per group (groupBy) <a id='18'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|group|\n",
      "+---+-----+\n",
      "|  0|    0|\n",
      "|  1|    1|\n",
      "|  2|    0|\n",
      "|  3|    1|\n",
      "|  4|    0|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [0,0],\n",
    "    [1,1],\n",
    "    [2,0],\n",
    "    [3,1],\n",
    "    [4,0]],(\"id\",\"group\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|group|max_id|\n",
      "+-----+------+\n",
      "|    0|     4|\n",
      "|    1|     3|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = (df_input\n",
    "             .groupby(\"group\")\n",
    "             .agg(max(\"id\").alias(\"max_id\")))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Collect values per group <a id='19'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|group|\n",
      "+---+-----+\n",
      "|  0|    0|\n",
      "|  1|    1|\n",
      "|  2|    0|\n",
      "|  3|    1|\n",
      "|  4|    0|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [0,0],\n",
    "    [1,1],\n",
    "    [2,0],\n",
    "    [3,1],\n",
    "    [4,0]],(\"id\",\"group\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|group|      ids|\n",
      "+-----+---------+\n",
      "|    0|[0, 2, 4]|\n",
      "|    1|   [1, 3]|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = df_input.groupby(\"group\").agg(collect_set(\"id\").alias(\"ids\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Multiple Aggregations <a id='20'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|group|\n",
      "+---+-----+\n",
      "|  0|    0|\n",
      "|  1|    1|\n",
      "|  2|    0|\n",
      "|  3|    1|\n",
      "|  4|    0|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [0,0],\n",
    "    [1,1],\n",
    "    [2,0],\n",
    "    [3,1],\n",
    "    [4,0]],(\"id\",\"group\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+\n",
      "|group|max_id|mind_id|\n",
      "+-----+------+-------+\n",
      "|    0|     4|      0|\n",
      "|    1|     3|      1|\n",
      "+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = df_input.groupby(\"group\").agg(max(\"id\").alias(\"max_id\"),min(\"id\").alias(\"mind_id\"))\n",
    "df_output.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
