{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICE TEST I: questions to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sobando\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EN2010333.endava.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1bb25f24370>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An Energy Company keeps detailed information about their customers. For an Machine Learning project, the project team is tasked with writing the most user friendly code.  To predict customer behavior patterns, team needs to create a Dataframe that needs to hold all customer details except sensitive info namely the Customer's SSN and Age. Choose the most effective option below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "|name|   address|gender|\n",
      "+----+----------+------+\n",
      "|John|  New York|     M|\n",
      "|Jill|California|     F|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['John','New York',25,'111-11-1111','M'],\n",
    "\n",
    "['Jill','California',24,'222-22-2222','F']],('name','address','age','ssn','gender'))\n",
    "\n",
    "df.drop(\"age\",\"ssn\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below is a Dataframe that has a field called Address but contains the city name alone. Below is the code to change the field name to 'City' appropriately. Choose the right option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+-----------+------+\n",
      "|name|      city|age|        ssn|gender|\n",
      "+----+----------+---+-----------+------+\n",
      "|John|  New York| 25|111-11-1111|     M|\n",
      "|Jill|California| 24|222-22-2222|     F|\n",
      "+----+----------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['John','New York',25,'111-11-1111','M'],\n",
    "\n",
    "['Jill','California',24,'222-22-2222','F']],('name','address','age','ssn','gender'))\n",
    "\n",
    "df.withColumnRenamed('address','city').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a Email Marketing company that sends automated emails to potential customers for up-selling products. The source data feeds for customer information has a single field for name that has a combined first & last name. For marketing needs the email needs to be sent out using last name alone.\n",
    "\n",
    "Below is the code to store the last name as a separate column. Choose the appropriate option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['Jane Miranda','Los Angeles','janem@mailcom'],\n",
    "\n",
    "['Lisa Smith','Chicago','lsmith@mail.com']],('name','city','email'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------------+-------+\n",
      "|        name|       city|          email|  Iname|\n",
      "+------------+-----------+---------------+-------+\n",
      "|Jane Miranda|Los Angeles|  janem@mailcom|Miranda|\n",
      "|  Lisa Smith|    Chicago|lsmith@mail.com|  Smith|\n",
      "+------------+-----------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Iname\",split(col(\"name\"),' ')[1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One of the Data transformation requirements is to normalize Age to a smaller number. Select the appropriate option from below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['John','Boston',30],\n",
    "\n",
    "['Smith','New York',20],\n",
    "\n",
    "                                               ['Sandra','Detroit',25],],('name','city','age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|Column|\n",
      "+------+------+\n",
      "|  John|     1|\n",
      "| Smith|     3|\n",
      "|Sandra|     2|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,when(df.age<25,3).when(df.age>25,1).otherwise(2).alias('Column')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the lowest salary in the below DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+\n",
      "|name|    city|salary|\n",
      "+----+--------+------+\n",
      "| Bob|  Boston| 10000|\n",
      "|Will|New York| 12000|\n",
      "|Lisa| Detroit| 13000|\n",
      "| Zoe| Detroit|  null|\n",
      "+----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['Bob','Boston',10000],\n",
    "\n",
    "['Will','New York',12000],\n",
    "\n",
    "                        ['Lisa','Detroit',13000],\n",
    "\n",
    "                        ['Zoe','Detroit',None],],('name','city','salary'))\n",
    "\n",
    "df.where(df.salary.isNotNull())\n",
    "\n",
    "df.withColumn(\"salary\",col(\"salary\")*1.02)\n",
    "\n",
    "df.where(df.name == 'Bob').withColumn(\"salary\",col(\"salary\")*1.5)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dentify the output of the code. Whose name comes up in the first Row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| null| 28|\n",
      "|  Joe| 25|\n",
      "|Smith| 27|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['Joe',25],\n",
    "\n",
    "['Smith',27],\n",
    "\n",
    "                                                [None,28],\n",
    "\n",
    "                                                ['Sharon',33]],('name','age'))\n",
    "\n",
    "df = df.filter(col(\"age\") < 30).filter(col(\"age\") > 20)\n",
    "\n",
    "df = df.select(\"*\").orderBy(col(\"name\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the output of the code. Whose name comes up in the first Row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Sharon|\n",
      "|  null|\n",
      "| Smith|\n",
      "|   Joe|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "                    ['Joe',25],\n",
    "\n",
    "                    ['Smith',27],\n",
    "\n",
    "                    [None,28],\n",
    "\n",
    "                    ['Sharon',33]],('name','score'))\n",
    "\n",
    "df.select(\"name\").orderBy(desc(\"score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the significance of the grouping clause below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------------+----------+\n",
      "|          dept|designation|grouping(dept)|sum(sales)|\n",
      "+--------------+-----------+--------------+----------+\n",
      "|          null|       null|             1|    105000|\n",
      "|SALES - RETAIL|     Sr Rep|             0|     50000|\n",
      "|SALES - ENERGY|     Sr Rep|             0|     25000|\n",
      "|SALES - ENERGY|       null|             0|     43000|\n",
      "|SALES - RETAIL|       null|             0|     62000|\n",
      "|SALES - ENERGY|     Jr Rep|             0|     18000|\n",
      "|SALES - RETAIL|     Jr Rep|             0|     12000|\n",
      "+--------------+-----------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "                    ['Will','SALES - ENERGY','Sr Rep',25000],\n",
    "\n",
    "                    ['Tom','SALES - RETAIL','Sr Rep',50000],\n",
    "\n",
    "                    ['Bill','SALES - RETAIL','Jr Rep',12000],\n",
    "\n",
    "                    ['Vanessa','SALES - ENERGY','Jr Rep',18000]],('name','dept','designation','sales'))\n",
    "\n",
    "df.rollup(\"dept\",\"designation\").agg(grouping(\"dept\"),sum(\"sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Dataframe consists of many records that don't have values in the department column. For a departmental salary analysis, the records without value in the department column needs to be removed. Choose the appropriate command to achieve this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',None,8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sales',5000],\n",
    "\n",
    "                    ['Tim','IT',7000]],('name','dept','salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "|Jill|Sales|  5000|\n",
      "| Tim|   IT|  7000|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the output of \"set_salary\" for \"HR\" dept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| dept|  set_salary|\n",
      "+-----+------------+\n",
      "|   HR|[8000, 5000]|\n",
      "|ADMIN|      [7000]|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['William',\"HR\",8000],\n",
    "\n",
    "                    ['Bob',\"HR\",8000],\n",
    "\n",
    "                    ['Lisa','HR',5000],\n",
    "\n",
    "                    ['Marie','ADMIN',7000]],('name','dept','salary'))\n",
    "\n",
    "dfEmp.groupBy(\"dept\").agg(collect_set(\"salary\").alias(\"set_salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When the below dataframe's are joined with each other, there should not be any data loss;all the records from both the dataframes should be displayed. What is the appropriate join option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+-----+----------------+\n",
      "|name| dept|salary| dept|       dept_name|\n",
      "+----+-----+------+-----+----------------+\n",
      "|Tony|   HR|  8000| null|            null|\n",
      "|Mona| null| 10000| null|            null|\n",
      "| Tim|Admin|  7000|Admin|  Administration|\n",
      "|null| null|  null|   IT|Information Tech|\n",
      "|Jill|  Sls|  5000|  Sls|           Sales|\n",
      "+----+-----+------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "\n",
    "\n",
    "dfDept = spark.createDataFrame([\n",
    "\n",
    "                    ['Sls','Sales'],\n",
    "\n",
    "                    ['Admin','Administration'],\n",
    "\n",
    "                    ['IT','Information Tech']],('dept','dept_name'))\n",
    "\n",
    "dfEmp.join(dfDept, dfEmp.dept == dfDept.dept, 'outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on code below, what is the \"profit_loss_ind\" for \"ABC\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------------+\n",
      "|company_name|profit_loss|profit_loss_ind|\n",
      "+------------+-----------+---------------+\n",
      "|         ABC|      20000|              P|\n",
      "|         XYZ|      10000|              P|\n",
      "+------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNetIncome = spark.createDataFrame([\n",
    "\n",
    "                    ['ABC',-20000],\n",
    "\n",
    "                    ['XYZ',10000]],('company_name','profit_loss'))\n",
    "\n",
    "dfNetIncome = dfNetIncome.withColumn(\"profit_loss\",abs(col(\"profit_loss\")))\n",
    "\n",
    "dfNetIncome = dfNetIncome.select(col(\"company_name\"),col(\"profit_loss\"),when(dfNetIncome.profit_loss < 0,\"L\").otherwise(\"P\").alias(\"profit_loss_ind\"))\n",
    "\n",
    "dfNetIncome.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the given code below, what is the \"salesRank\" for \"Headphones\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---------+\n",
      "|  itemName|sales_quantity|salesRank|\n",
      "+----------+--------------+---------+\n",
      "|        TV|           200|        1|\n",
      "|    Phones|           300|        2|\n",
      "|    Office|           300|        2|\n",
      "|Headphones|           400|        4|\n",
      "|   Kitchen|           500|        5|\n",
      "+----------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "dfSales= spark.createDataFrame([\n",
    "\n",
    "                    ['TV',200],\n",
    "\n",
    "                    ['Headphones',400],\n",
    "\n",
    "                    ['Phones',300],\n",
    "\n",
    "                    ['Kitchen',500],\n",
    "\n",
    "                    ['Office',300]],('itemName','sales_quantity'))\n",
    "\n",
    "\n",
    "\n",
    "windowSpec = Window.orderBy(\"sales_quantity\")\n",
    "\n",
    "salesRank = rank().over(windowSpec)\n",
    "\n",
    "dfSales.select(\n",
    "\n",
    "       col(\"itemName\"),\n",
    "\n",
    "       col(\"sales_quantity\"),\n",
    "\n",
    "       salesRank.alias(\"salesRank\")\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the output of the below code (in relation to the current date)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+----------+\n",
      "|name| dept|salary|  date_col|\n",
      "+----+-----+------+----------+\n",
      "|Tony| null|  8000|2020-01-01|\n",
      "|Mona| null| 10000|2020-01-01|\n",
      "|Jill|Sales|  5000|2020-01-01|\n",
      "| Tim|   IT|  7000|2020-01-01|\n",
      "+----+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"date_col\",trunc(current_date(),\"yyyy\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the correct output of below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-d7f6c5376add>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"John\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Column is not iterable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;31m# string methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "df.select(map(lit(\"1\"), lit(\"John\"))).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the output of \"set_salary\" for \"HR\" dept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| dept|  set_salary|\n",
      "+-----+------------+\n",
      "|   HR|[8000, 5000]|\n",
      "|ADMIN|      [7000]|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['William',\"HR\",8000],\n",
    "\n",
    "                    ['Bob',\"HR\",8000],\n",
    "\n",
    "                    ['Lisa','HR',5000],\n",
    "\n",
    "                    ['Marie','ADMIN',7000]],('name','dept','salary'))\n",
    "\n",
    "dfEmp.groupBy(\"dept\").agg(collect_set(\"salary\").alias(\"set_salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the correct option below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  keys|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(create_map(lit(\"1\"), lit(\"John\"),lit(\"2\"),lit(\"Marie\")).alias(\"empDetails\")).limit(1)\n",
    "\n",
    "df.select(map_keys(\"empDetails\").alias(\"keys\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are Spark Executors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are responsible for running Application Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a requirement to join all data between 2 Dataframes, one Dataframe based on billions of sales records and the other Dataframe with less than 100,000 records with the Dept Names and Dept IDs of departments. The common key between the two Dataframes is Dept ID. From a performance standpoint, what is the ideal strategy from given options below?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast the samller DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many columns will the below code store in output TEXT file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Text data source does not support bigint data type.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f785b6e8ef36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"/Temp/Data/empDetails.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdfEmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[1;34m(self, path, compression, lineSep)\u001b[0m\n\u001b[0;32m    952\u001b[0m         \"\"\"\n\u001b[0;32m    953\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Text data source does not support bigint data type.;"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "dfEmp = dfEmp.drop(\"dept\")\n",
    "\n",
    "dfEmp = dfEmp.withColumn(\"commision\",col(\"salary\")*1.03)\n",
    "\n",
    "path=\"/Temp/Data/empDetails.txt\"\n",
    "\n",
    "dfEmp.write.text(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An Retail company processes its sales in Apache Spark on a daily batch basis. Each day, the sales is added into \"sales_details.parquet\" file that contains historical and on-going daily sales. what is the most appropriate option to ensure that the sales are loaded without failures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mode == append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What does the \"True\" option below refer to?\n",
    "\n",
    "```PYTHON\n",
    "SCHEMA = StructType([\n",
    "\n",
    "            StructField(\"id\",IntegerType(),True),\n",
    "\n",
    "            StructField(\"name\",StringType(),True)\n",
    "\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Column can have empty values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the appropriate number of columns displayed by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-2cfb652f5bdf>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-2cfb652f5bdf>\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    dfEmp.show()nRenamed(\"name\",\"first_name\")\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "SCHEMA = StructType([\n",
    "\n",
    "            StructField(\"name\",StringType(),True),\n",
    "\n",
    "            StructField(\"dept\",StringType(),True),\n",
    "\n",
    "            StructField(\"daily_commision\",IntegerType(),True)\n",
    "\n",
    "])\n",
    "\n",
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",85.65],\n",
    "\n",
    "                    ['Tim','Admin',94.75]],SCHEMA)\n",
    "\n",
    "dfEmp = dfEmp.withColumn(\"designation\",lit(\"TBD\"))\n",
    "\n",
    "dfEmp.drop(\"dept\")\n",
    "\n",
    "dfEmp = dfEmp.withColumnRenamed(\"name\",\"first_name\")\n",
    "\n",
    "dfEmp.show()nRenamed(\"name\",\"first_name\")\n",
    "\n",
    "dfEmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the below code, estimate the salary of Mona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|name|test_comm(salary)|\n",
      "+----+-----------------+\n",
      "|Tony|           8400.0|\n",
      "|Mona|          10500.0|\n",
      "|Jill|           5250.0|\n",
      "| Tim|           7350.0|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "def test_comm(x):\n",
    "\n",
    "  return x * 1.05\n",
    "\n",
    "testcomm = udf(test_comm)\n",
    "\n",
    "dfEmp.select(col(\"name\"),testcomm(col(\"salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark 2.X vs Spark 3.X :- Which of the following statement is True?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 2.X does not recognize GPU accelerator; Spark 3.X recognizes GPU accelerator natively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the output of the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+----------+----------+\n",
      "|name| dept|salary|  date_col|  last_day|\n",
      "+----+-----+------+----------+----------+\n",
      "|Tony| null|  8000|2020-01-01|2020-12-31|\n",
      "|Mona| null| 10000|2020-01-01|2020-12-31|\n",
      "|Jill|Sales|  5000|2020-01-01|2020-12-31|\n",
      "| Tim|   IT|  7000|2020-01-01|2020-12-31|\n",
      "+----+-----+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"last_day\",last_day(current_date())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An Email Marketing campaign company gets customer information from various clients. In some cases, the clients send the zip code as an Integer that leads to the loss of 0's in the beginning of zip codes less than 5 numbers. The task is to add those 0's back again. What is the most efficient method to add the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ipad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the correct output of below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-54ab87fdcc1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"John\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Column is not iterable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;31m# string methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "df.select(map(lit(\"1\"), lit(\"John\"))).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the correct output of below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|array(1, John)|\n",
      "+--------------+\n",
      "|     [1, John]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(array(lit(\"1\"),lit(\"John\"))).limit(1).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
