{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Apache Sparkâ„¢ and Scala Workshops PART - I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are my own solutions version in PySpark of the Exercises proposed by Jacek Laskowski in https://github.com/jaceklaskowski/spark-workshop/tree/gh-pages/exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "1. [Split function with variable delimiter per row](#1) \n",
    "2. [Selecting the most important rows per assigned priority](#2) \n",
    "3. [Adding count to the source DataFrame](#3) \n",
    "4. [Limiting collect_set Standard Function](#4) \n",
    "5. [Structs for column names and values](#5) \n",
    "6. [Merging two rows](#6)\n",
    "7. [Exploding structs array](#7)\n",
    "8. [Finding Ids of Rows with Word in Array Column](#8)\n",
    "9. [Using Dataset.flatMap Operator](#9)\n",
    "10.[Flattening Array Columns (From Datasets of Arrays to Datasets of Array Elements)](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sobando\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EN2010333.endava.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2148ecc8430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split function with variable delimiter per row <a id='1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|             VALUES|Delimiter|\n",
      "+-------------------+---------+\n",
      "|       50000.0#0#0#|        #|\n",
      "|          0@1000.0@|        @|\n",
      "|                 1$|        $|\n",
      "|1000.00^Test_string|        ^|\n",
      "+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame Input\n",
    "dept = spark.createDataFrame([\n",
    "  [\"50000.0#0#0#\", \"#\"],\n",
    "  [\"0@1000.0@\", \"@\"],\n",
    "  [\"1$\", \"$\"],\n",
    "  [\"1000.00^Test_string\", \"^\"]],(\"VALUES\", \"Delimiter\"))\n",
    "# For the script version\n",
    "dept_sql = dept\n",
    "\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using UDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------------------+--------------------+\n",
      "|             VALUES|Delimiter|        split_values|               extra|\n",
      "+-------------------+---------+--------------------+--------------------+\n",
      "|       50000.0#0#0#|        #|   [50000.0, 0, 0, ]|     [50000.0, 0, 0]|\n",
      "|          0@1000.0@|        @|       [0, 1000.0, ]|         [0, 1000.0]|\n",
      "|                 1$|        $|               [1, ]|                 [1]|\n",
      "|1000.00^Test_string|        ^|[1000.00, Test_st...|[1000.00, Test_st...|\n",
      "+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame Output API\n",
    "## Declare functions\n",
    "def splitbydelimiter(array,delimiter):\n",
    "\n",
    "  return array.split(delimiter)\n",
    "\n",
    "def removeblank(array):\n",
    "  #return [x for x in array if x] with list comprehension\n",
    "  return list(filter(None, array))\n",
    "## Set UDFs\n",
    "splitbydelimiter = udf(splitbydelimiter)\n",
    "removeblank = udf(removeblank)\n",
    "## Apply\n",
    "dept = dept.withColumn('split_values',splitbydelimiter(col(\"VALUES\"),col(\"Delimiter\"))) ## Split by special character and add the array as column 'split_values'\n",
    "dept = dept.withColumn('extra',removeblank(col('split_values'))) ## Remove blank from array as column 'extra'\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using UDFs with SQL scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------------------+--------------------+\n",
      "|             VALUES|Delimiter|        split_values|               extra|\n",
      "+-------------------+---------+--------------------+--------------------+\n",
      "|       50000.0#0#0#|        #|   [50000.0, 0, 0, ]|     [50000.0, 0, 0]|\n",
      "|          0@1000.0@|        @|       [0, 1000.0, ]|         [0, 1000.0]|\n",
      "|                 1$|        $|               [1, ]|                 [1]|\n",
      "|1000.00^Test_string|        ^|[1000.00, Test_st...|[1000.00, Test_st...|\n",
      "+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame Output SQL\n",
    "## Declare table\n",
    "dept_sql.createOrReplaceTempView('dept_sql')\n",
    "## Set UDFs\n",
    "spark.udf.register(\"splitbydelimiter\", splitbydelimiter)\n",
    "spark.udf.register(\"removeblank\", removeblank)\n",
    "## Define script\n",
    "script = \"SELECT *, removeblank(split_values) as extra FROM ( \\\n",
    "          SELECT VALUES, Delimiter, splitbydelimiter(VALUES,Delimiter) as split_values FROM dept_sql);\"\n",
    "## Apply\n",
    "dept_sql = spark.sql(script)\n",
    "dept_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selecting the most important rows per assigned priority <a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| value|\n",
      "+---+------+\n",
      "|  1|   MV1|\n",
      "|  1|   MV2|\n",
      "|  2|   VPV|\n",
      "|  2|Others|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame Input\n",
    "df_input = spark.createDataFrame([\n",
    "  [1, \"MV1\"],\n",
    "  [1, \"MV2\"],\n",
    "  [2, \"VPV\"],\n",
    "  [2, \"Others\"]],(\"id\", \"value\"))\n",
    "# For the script version\n",
    "df_input_sql = df_input\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|  MV1|\n",
      "|  2|  VPV|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame Output API\n",
    "df_input.dropDuplicates(subset=[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|  MV1|\n",
      "|  2|  VPV|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another Approach\n",
    "window = Window.partitionBy(\"id\").orderBy(\"id\",'tiebreak')\n",
    "(df_input\n",
    " .withColumn('tiebreak', monotonically_increasing_id())\n",
    " .withColumn('rank', rank().over(window))\n",
    " .filter(col('rank') == 1).drop('rank','tiebreak')\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| id|value|row_number|\n",
      "+---+-----+----------+\n",
      "|  1|  MV1|         1|\n",
      "|  2|  VPV|         1|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "# DataFrame Output SQL\n",
    "## Declare table\n",
    "df_input_sql.createOrReplaceTempView('df_input_sql')\n",
    "## Define script\n",
    "script = \" \\\n",
    "SELECT * \\\n",
    "FROM ( \\\n",
    "  SELECT \\\n",
    "      *, \\\n",
    "      ROW_NUMBER() \\\n",
    "          OVER (PARTITION BY id ORDER BY id) \\\n",
    "          row_number \\\n",
    "  FROM df_input_sql \\\n",
    ") \\\n",
    "WHERE row_number = 1;\"\n",
    "\n",
    "## Apply\n",
    "spark.sql(script).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding count to the source DataFrame <a id='3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+-----+\n",
      "|        column0|       column1|       column2|label|\n",
      "+---------------+--------------+--------------+-----+\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|\n",
      "+---------------+--------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "  [\"05:49:56.604899\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604900\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604899\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604900\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604899\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604900\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604899\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604900\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604899\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604900\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604899\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604900\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604899\", \"10.0.0.2.54880\", \"10.0.0.3.5001\",  2],\n",
    "  [\"05:49:56.604908\", \"10.0.0.3.5001\",  \"10.0.0.2.54880\", 2],\n",
    "  [\"05:49:56.604908\", \"10.0.0.3.5001\",  \"10.0.0.2.54880\", 2],\n",
    "  [\"05:49:56.604908\", \"10.0.0.3.5001\",  \"10.0.0.2.54880\", 2],\n",
    "  [\"05:49:56.604908\", \"10.0.0.3.5001\",  \"10.0.0.2.54880\", 2],\n",
    "  [\"05:49:56.604908\", \"10.0.0.3.5001\",  \"10.0.0.2.54880\", 2],\n",
    "  [\"05:49:56.604908\", \"10.0.0.3.5001\",  \"10.0.0.2.54880\", 2],\n",
    "  [\"05:49:56.604908\", \"10.0.0.3.5001\",  \"10.0.0.2.54880\", 2]],(\"column0\", \"column1\", \"column2\", \"label\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+-----+-----+\n",
      "|        column0|       column1|       column2|label|count|\n",
      "+---------------+--------------+--------------+-----+-----+\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "+---------------+--------------+--------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create aggregation table\n",
    "df_input_agg = df_input.select(col(\"column2\")).groupby(\"column2\").count()\n",
    "df_input_agg = df_input_agg.withColumnRenamed(\"column2\",\"key\")\n",
    "# Join\n",
    "df_input.join(df_input_agg,df_input[\"column2\"]==df_input_agg[\"key\"],\"left\").drop(\"key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+-----+-----+\n",
      "|        column0|       column1|       column2|label|count|\n",
      "+---------------+--------------+--------------+-----+-----+\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604900|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604899|10.0.0.2.54880| 10.0.0.3.5001|    2|   13|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "|05:49:56.604908| 10.0.0.3.5001|10.0.0.2.54880|    2|    7|\n",
      "+---------------+--------------+--------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input.createOrReplaceTempView(\"df_input_agg\")\n",
    "script = \"\\\n",
    "SELECT\\\n",
    "    column0,\\\n",
    "    column1,\\\n",
    "    column2,\\\n",
    "    label,\\\n",
    "    count \\\n",
    "FROM\\\n",
    "    df_input_agg \\\n",
    "LEFT JOIN \\\n",
    "(SELECT\\\n",
    "    column2 AS key_agg,\\\n",
    "        count(column2) AS count\\\n",
    "     FROM\\\n",
    "        df_input_agg\\\n",
    "    GROUP BY\\\n",
    "        1) \\\n",
    "ON\\\n",
    "    column2 == key_agg;\"\n",
    "spark.sql(script).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Limiting collect_set Standard Function <a id='4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|key|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "|  5|  0|\n",
      "|  6|  1|\n",
      "|  7|  2|\n",
      "|  8|  3|\n",
      "|  9|  4|\n",
      "| 10|  0|\n",
      "| 11|  1|\n",
      "| 12|  2|\n",
      "| 13|  3|\n",
      "| 14|  4|\n",
      "| 15|  0|\n",
      "| 16|  1|\n",
      "| 17|  2|\n",
      "| 18|  3|\n",
      "| 19|  4|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "[0,0],\n",
    "[1,1],\n",
    "[2,2],\n",
    "[3,3],\n",
    "[4,4],\n",
    "[5,0],\n",
    "[6,1],\n",
    "[7,2],\n",
    "[8,3],\n",
    "[9,4],\n",
    "[10,0],\n",
    "[11,1],\n",
    "[12,2],\n",
    "[13,3],\n",
    "[14,4],\n",
    "[15,0],\n",
    "[16,1],\n",
    "[17,2],\n",
    "[18,3],\n",
    "[19,4],\n",
    "[20,0],\n",
    "[21,1],\n",
    "[22,2],\n",
    "[23,3],\n",
    "[24,4],\n",
    "[25,0],\n",
    "[26,1],\n",
    "[27,2],\n",
    "[28,3],\n",
    "[29,4],\n",
    "[30,0],\n",
    "[31,1],\n",
    "[32,2],\n",
    "[33,3],\n",
    "[34,4],\n",
    "[35,0],\n",
    "[36,1],\n",
    "[37,2],\n",
    "[38,3],\n",
    "[39,4],\n",
    "[40,0],\n",
    "[41,1],\n",
    "[42,2],\n",
    "[43,3],\n",
    "[44,4],\n",
    "[45,0],\n",
    "[46,1],\n",
    "[47,2],\n",
    "[48,3],\n",
    "[49,4],\n",
    "],(\"id\",\"key\"))\n",
    "\n",
    "df_input.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+\n",
      "|key|                 all|only_first_three|\n",
      "+---+--------------------+----------------+\n",
      "|  0|[0, 15, 30, 45, 5...|     [0, 15, 30]|\n",
      "|  1|[1, 16, 31, 46, 6...|     [1, 16, 31]|\n",
      "|  3|[33, 48, 13, 38, ...|    [33, 48, 13]|\n",
      "|  2|[12, 27, 37, 2, 1...|    [12, 27, 37]|\n",
      "|  4|[9, 19, 34, 49, 2...|     [9, 19, 34]|\n",
      "+---+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_input.groupby(\"key\").agg(collect_set(\"id\").alias(\"all\")) # Get All\n",
    "df = df.withColumn(\"only_first_three\",slice(col(\"all\"),1,3)) # Get first three elements\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Limiting collect_set Standard Function <a id='5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  name|        movieRatings|\n",
      "+------+--------------------+\n",
      "|Manuel|[[Logan, 1.5], [Z...|\n",
      "|  John|[[Logan, 2.0], [Z...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [\"Manuel\",[[\"Logan\",1.5],[\"Zoolander\",3.0], [\"John Wick\",2.5]]],\n",
    "    [\"John\",[[\"Logan\",2.0], [\"Zoolander\",3.5], [\"John Wick\",3.0]]],\n",
    "],(\"name\",\"movieRatings\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+---------+\n",
      "|  name|John Wick|Logan|Zoolander|\n",
      "+------+---------+-----+---------+\n",
      "|  John|      3.0|  2.0|      3.5|\n",
      "|Manuel|      2.5|  1.5|      3.0|\n",
      "+------+---------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode array\n",
    "df_output = (df_input\n",
    "             .withColumn(\"explode\",explode(col(\"movieRatings\")))\n",
    "             .drop(\"movieRatings\"))\n",
    "# Get array elements into columns\n",
    "df_output = (df_output\n",
    "             .withColumn(\"Movie\",element_at(\"explode\",1))\n",
    "             .withColumn(\"Rate\",element_at(\"explode\",2))\n",
    "             .drop(\"explode\"))\n",
    "# Transpose de table by pivot it\n",
    "df_output = (df_output\n",
    "             .groupby(\"name\")\n",
    "             .pivot(\"Movie\")\n",
    "             .agg(sum(\"Rate\")))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merging two rows <a id='6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+--------+\n",
      "| id| name| age|    city|\n",
      "+---+-----+----+--------+\n",
      "|100| John|  35|    null|\n",
      "|100| John|null| Georgia|\n",
      "|101| Mike|  25|    null|\n",
      "|101| Mike|null|New York|\n",
      "|103| Mary|  22|    null|\n",
      "|103| Mary|null|   Texas|\n",
      "|104|Smith|  25|    null|\n",
      "|105| Jake|null| Florida|\n",
      "+---+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "  [\"100\",\"John\", 35,None],\n",
    "  [\"100\",\"John\", None,\"Georgia\"],\n",
    "  [\"101\",\"Mike\", 25,None],\n",
    "  [\"101\",\"Mike\", None,\"New York\"],\n",
    "  [\"103\",\"Mary\", 22,None],\n",
    "  [\"103\",\"Mary\", None,\"Texas\"],\n",
    "  [\"104\",\"Smith\", 25,None],\n",
    "  [\"105\",\"Jake\", None,\"Florida\"]],(\"id\", \"name\", \"age\", \"city\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+--------+\n",
      "| id| name| age|    city|\n",
      "+---+-----+----+--------+\n",
      "|100| John|  35| Georgia|\n",
      "|101| Mike|  25|New York|\n",
      "|103| Mary|  22|   Texas|\n",
      "|104|Smith|  25|    null|\n",
      "|105| Jake|null| Florida|\n",
      "+---+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get first value not null for each column in order to group by without the less posible amount of nulls\n",
    "df_output = (df_input\n",
    "             .groupBy(\"id\",\"name\")\n",
    "             .agg(*[first(x,ignorenulls=True) for x in df_input.columns if x!='id' and x!='name']))\n",
    "# Rename columns\n",
    "df_output = (df_output\n",
    "             .withColumnRenamed(\"first(age)\",\"age\")\n",
    "             .withColumnRenamed(\"first(city)\",\"city\")\n",
    "             .orderBy(\"id\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploding structs array <a id='7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+\n",
      "|business_id|  full_address|               hours|\n",
      "+-----------+--------------+--------------------+\n",
      "|        abc|random_address|[[02:00, 11:00], ...|\n",
      "+-----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.read.json(\"exercise_7.json\")\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+---------+---------+----------+\n",
      "|business_id|  full_address|      day|open_time|close_time|\n",
      "+-----------+--------------+---------+---------+----------+\n",
      "|        abc|random_address|   Friday|    11:00|     02:00|\n",
      "|        abc|random_address|   Monday|    11:00|     02:00|\n",
      "|        abc|random_address| Saturday|    11:00|     02:00|\n",
      "|        abc|random_address|   Sunday|    11:00|     00:00|\n",
      "|        abc|random_address| Thursday|    11:00|     02:00|\n",
      "|        abc|random_address|  Tuesday|    11:00|     02:00|\n",
      "|        abc|random_address|Wednesday|    11:00|     02:00|\n",
      "+-----------+--------------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get struct days and hours\n",
    "df_output = df_input.select(col(\"business_id\"),col(\"full_address\"),col(\"hours.*\"))\n",
    "# Unpivot the table and get for separated day and hours\n",
    "df_output = df_output.selectExpr(\"business_id\",\"full_address\",\"stack(7,'Friday',Friday,'Monday',Monday,'Saturday',Saturday,'Sunday',Sunday,'Thursday',Thursday,'Tuesday',Tuesday,'Wednesday',Wednesday) as (day, hour)\")\n",
    "# Get open and close time from struct hour\n",
    "df_output = (df_output\n",
    "             .withColumn(\"open_time\",col(\"hour\").getField(\"open\"))\n",
    "             .withColumn(\"close_time\",col(\"hour\").getField(\"close\"))\n",
    "             .drop(\"hour\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Finding Ids of Rows with Word in Array Column <a id='8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|             words| word|\n",
      "+---+------------------+-----+\n",
      "|  1|     one,two,three|  one|\n",
      "|  2|     four,one,five|  six|\n",
      "|  3|seven,nine,one,two|eight|\n",
      "|  4|    two,three,five| five|\n",
      "|  5|      six,five,one|seven|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,\"one,two,three\",\"one\"],\n",
    "    [2,\"four,one,five\",\"six\"],\n",
    "    [3,\"seven,nine,one,two\",\"eight\"],\n",
    "    [4,\"two,three,five\",\"five\"],\n",
    "    [5,\"six,five,one\",\"seven\"],\n",
    "],(\"id\",\"words\",\"word\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| word|         ids|\n",
      "+-----+------------+\n",
      "| five|   [5, 2, 4]|\n",
      "|  one|[1, 5, 2, 3]|\n",
      "|seven|         [3]|\n",
      "|  six|         [5]|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get words as a array\n",
    "df_output = df_input.withColumn(\"split\",split(col(\"words\"),\",\")).drop(\"words\").drop(\"word\")\n",
    "# Flatten array\n",
    "df_output = df_output.withColumn(\"explode\",explode(\"split\")).drop(\"split\")\n",
    "# Get wor as key\n",
    "df_input_used = df_input.select(\"word\")\n",
    "# Join Keys with flatten words\n",
    "df_output = df_input_used.join(df_output,df_input.word == df_output.explode,\"inner\")\n",
    "# Select main columns\n",
    "df_output = df_output.select(\"word\",\"id\")\n",
    "# Collect ids\n",
    "df_output = df_output.groupby(\"word\").agg(collect_set(\"id\").alias(\"ids\")).orderBy(\"word\")\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Dataset.flatMap Operator <a id='9'></a>\n",
    "\n",
    "There is not datasets on Python, este mÃ©todo es mÃ¡s eficiente que el explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     nums|\n",
      "+---+---------+\n",
      "|  1|[1, 2, 3]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "    [1,[1,2,3]],\n",
    "],(\"id\",\"nums\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+\n",
      "| id|     nums|explode|\n",
      "+---+---------+-------+\n",
      "|  1|[1, 2, 3]|      1|\n",
      "|  1|[1, 2, 3]|      2|\n",
      "|  1|[1, 2, 3]|      3|\n",
      "+---+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output = df_input.withColumn(\"explode\",explode(\"nums\"))\n",
    "df_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Flattening Array Columns (From Datasets of Arrays to Datasets of Array Elements) <a id='10'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|    value|\n",
      "+---+---------+\n",
      "|  1|[a, b, c]|\n",
      "|  2|[X, Y, Z]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_input = spark.createDataFrame([\n",
    "  [1,[\"a\",\"b\",\"c\"]],\n",
    "  [2,[\"X\",\"Y\",\"Z\"]]],(\"id\",\"value\"))\n",
    "\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  0|  1|  2|\n",
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "|  X|  Y|  Z|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First approach\n",
    "df_output = (df_input\n",
    "             .withColumn(\"0\",element_at(\"value\",1))\n",
    "             .withColumn(\"1\",element_at(\"value\",2))\n",
    "             .withColumn(\"2\",element_at(\"value\",3))\n",
    "             .drop(\"id\",\"value\"))\n",
    "df_output.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
