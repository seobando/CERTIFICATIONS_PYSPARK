{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICE TEST II: questions to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\sobando\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The below Dataframe's need to joined with each other in same order below and the output should display all records from dfDept which has no match in the dfEmp Dataframe. What is the appropriate join option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "dfDept = spark.createDataFrame([\n",
    "\n",
    "                    ['Sls','Sales'],\n",
    "\n",
    "                    ['Admin','Administration'],\n",
    "\n",
    "                    ['IT','Information Tech']],('dept','dept_name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|name|dept|salary|\n",
      "+----+----+------+\n",
      "|Tony|  HR|  8000|\n",
      "|Mona|null| 10000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp.join(dfDept,dfEmp.dept == dfDept.dept,how=\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| Tim|Admin|  7000|\n",
      "|Jill|  Sls|  5000|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp.join(dfDept,dfEmp.dept == dfDept.dept,how=\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmp.createOrReplaceTempView('Emp')\n",
    "dfDept.createOrReplaceTempView('Dept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|name|dept|salary|\n",
      "+----+----+------+\n",
      "|Tony|  HR|  8000|\n",
      "|Mona|null| 10000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL script\n",
    "sql = \"SELECT * FROM Emp ANTI JOIN Dept ON Emp.dept = Dept.dept;\" \n",
    "# Spark SQL\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|name|dept|salary|\n",
      "+----+----+------+\n",
      "|Tony|  HR|  8000|\n",
      "|Mona|null| 10000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL script\n",
    "sql = \"SELECT * FROM Emp ANTI JOIN Dept ON Emp.dept = Dept.dept;\" \n",
    "# Spark SQL\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference = https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records will the below query display?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+----------------+\n",
      "| dept|name|salary|       dept_name|\n",
      "+-----+----+------+----------------+\n",
      "|   HR|Tony|  8000|            null|\n",
      "| null|Mona| 10000|            null|\n",
      "|Admin| Tim|  7000|  Administration|\n",
      "|   IT|null|  null|Information Tech|\n",
      "|  Sls|Jill|  5000|           Sales|\n",
      "+-----+----+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "dfDept = spark.createDataFrame([\n",
    "\n",
    "                    ['Sls','Sales'],\n",
    "\n",
    "                    ['Admin','Administration'],\n",
    "\n",
    "                    ['IT','Information Tech']],('dept','dept_name'))\n",
    "\n",
    "df = dfEmp.join(dfDept,\"dept\",\"outer\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+--------------+\n",
      "| dept|name|salary|     dept_name|\n",
      "+-----+----+------+--------------+\n",
      "|Admin| Tim|  7000|Administration|\n",
      "|  Sls|Jill|  5000|         Sales|\n",
      "+-----+----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.na.drop(\"any\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A source data file has 5 columns. One of the columns has bad data and will cause the read to fail for a few records. In order to ensure that the file reads successfully for the good quality records and keep track of the bad data to analyze further, what is the appropriate option below?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columnNameOfCorruptRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The source data to be read into a DataFrame exists in a SQL Database. In order to reduce the number of records processed by the Dataframe, we need to reduce the number of records sourced from the read action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set upperbpund, lowerbound, numPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How many columns will the write command output in CSV file, based on last line of code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/sobando/Downloads/SPARK_PRACTICE/empDetails.csv already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-6d99adb1a39a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdfEmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfEmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dept\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdfEmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1025\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1027\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/sobando/Downloads/SPARK_PRACTICE/empDetails.csv already exists.;"
     ]
    }
   ],
   "source": [
    "dfEmp = spark.createDataFrame([\n",
    "\n",
    "                    ['Tony',\"HR\",8000],\n",
    "\n",
    "                    ['Mona',None,10000],\n",
    "\n",
    "                    ['Jill','Sls',5000],\n",
    "\n",
    "                    ['Tim','Admin',7000]],('name','dept','salary'))\n",
    "\n",
    "path=\"empDetails.csv\"\n",
    "\n",
    "dfEmp = dfEmp.withColumn(\"commision\",col(\"salary\")*1.03)\n",
    "\n",
    "dfEmp.write.csv(path)\n",
    "\n",
    "dfEmp = dfEmp.drop(\"dept\")\n",
    "\n",
    "dfEmp.write.csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. An Retail company processes its sales in Apache Spark on a daily batch basis. Each day, the sales is added into \"sales_details.parquet\" file that contains historical and on-going daily sales. what is the most appropriate compression option for parquet files to have optimal performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "No es claro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. There is a Dataframe that has 30+ Columns. For further analysis, we need a specific set of 3 columns to be selected for further analysis. All options given below are suitable for the requirement except one. Choose the invalid option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.select(cols(\"fname\",\"lname\",\"sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Choose the correct option to display the contents of the Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "cust = Row(\"fname\",\"lname\")\n",
    "\n",
    "custRow = cust(\"John\",\"Doe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('John', 'Doe')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custRow[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. The Dataframe df consists of information related to sales dept. Due to sales team hitting their targets, the management has provided a directive to assign 5% commission. Select the appropriate option from below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "\n",
    "['John','Doe','25','2800'],\n",
    "\n",
    "['Joe','Smith','24','2600']],('fname','lname','age','sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-----+---------+\n",
      "|fname|lname|age|sales|sales_com|\n",
      "+-----+-----+---+-----+---------+\n",
      "| John|  Doe| 25| 2800|    140.0|\n",
      "|  Joe|Smith| 24| 2600|    130.0|\n",
      "+-----+-----+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df.withColumn(\"sales_com\",col(\"sales\").cast(IntegerType())*0.05).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. There are different methods to create a Dataframe in order run small tests. One of the options below will not provide the desired output. Choose the incorrect option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['John','Boston','john@mailcom'],\n",
    "    ['Smith','New York','smith@mail.com']\n",
    "],('name','city','email'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------+\n",
      "| name|    city|         email|\n",
      "+-----+--------+--------------+\n",
      "| John|  Boston|  john@mailcom|\n",
      "|Smith|New York|smith@mail.com|\n",
      "+-----+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['John','Boston','john@mailcom'],\n",
    "    ['Smith','New York','smith@mail.com']\n",
    "]).toDF('name','city','email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------+\n",
      "| name|    city|         email|\n",
      "+-----+--------+--------------+\n",
      "| John|  Boston|  john@mailcom|\n",
      "|Smith|New York|smith@mail.com|\n",
      "+-----+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "withColumnRenamed() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e04d3e1b686d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df = spark.createDataFrame([\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;34m'John'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Boston'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'john@mailcom'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;34m'Smith'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'New York'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'smith@mail.com'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ]).withColumnRenamed(col('name'),col('city'),col('email'))\n",
      "\u001b[1;31mTypeError\u001b[0m: withColumnRenamed() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['John','Boston','john@mailcom'],\n",
    "    ['Smith','New York','smith@mail.com']\n",
    "]).withColumnRenamed(col('name'),col('city'),col('email'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=['name','city','email']\n",
    "df = spark.createDataFrame([\n",
    "    ['John','Boston','john@mailcom'],\n",
    "    ['Smith','New York','smith@mail.com']\n",
    "],lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------+\n",
      "| name|    city|         email|\n",
      "+-----+--------+--------------+\n",
      "| John|  Boston|  john@mailcom|\n",
      "|Smith|New York|smith@mail.com|\n",
      "+-----+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. As part of existing Spark Code, there is a Global View \"gvSales\" that has been created. In order to select data from the global view, choose the appropriate option from below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
